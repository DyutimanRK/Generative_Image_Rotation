{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLvhBZS8hm2V"
   },
   "source": [
    "# Generative Network to rotate and obtain face-on PPD images\n",
    "## SOURCE CODE\n",
    "### Without Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This notebook is dedicated to training a Conditional Generative Adversarial Network (cGAN), incorporating the Pix2Pix concept, on a dataset of Protoplanetary Disk images obtained from the FARGO3D simulations. The primary goal of this project is to generate face-on images of Protoplanetary Disks from images that are rotated in random orientations.\n",
    "\n",
    "    \n",
    "    Summary       : The aim of this notebook is to leverage the power of cGANs, particularly the Pix2Pix\n",
    "                    architecture, to transform images of Protoplanetary Disks with random orientations into \n",
    "                    standardized face-on images. This transformation is crucial for various astronomical \n",
    "                    studies and simulations where consistent orientation of disk images is required for \n",
    "                    accurate analysis.\n",
    "\n",
    "\n",
    "    Code&Config   : The code is being done on Jupyter Notebook platform, and is being run on MacOS 13.1, \n",
    "                    Apple M1, 8gb configuration.  \n",
    "               \n",
    "         \n",
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervisor   : Dr. Sayantan Auddy\n",
    "#### Written by    : Dyutiman Santra\n",
    "#### Updated       : 26th June, 2024\n",
    "\n",
    "_______________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||  Importing Modules  ||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fUE7t0Oyhngi"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import glob \n",
    "\n",
    "from math import ceil\n",
    "\n",
    "#*****************\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#*****************\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import expand_dims\n",
    "from numpy import ones\n",
    "from numpy import zeros\n",
    "from numpy.random import randint\n",
    "\n",
    "#*****************\n",
    "\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from datetime import datetime as dt\n",
    "from IPython import display\n",
    "\n",
    "#*****************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||  Checking the availbale number of GPUs  ||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "Name: /physical_device:GPU:0 Type: GPU\n",
      "TensorFlow version used  2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\",gpu.name,\"Type:\",gpu.device_type)\n",
    "print(\"TensorFlow version used \",tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To specify our requirements\n",
    "\n",
    "no_o_folders = 700        #to set the number of folders (total 700)\n",
    "pxl = 128                #to set the desired pixel value\n",
    "channel = 1             #to set channel, 1 for grayscale and 3 for coloured\n",
    "save_interval = 20      #choosing the epoch interval to save model\n",
    "\n",
    "import ImageRotation_NetworkScript as NN\n",
    "\n",
    "csv_directory = '/Users/Dyutiman/Documents/ML_Project/Pix2Pix/RT_Dataset_incl_posang.csv'\n",
    "image_directory = '/Users/Dyutiman/Documents/ML_Project/Sayantan Da Projects/Final 1.5l'\n",
    "sound_file_directory = '/Users/Dyutiman/Downloads/terminating alarm.mp3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||  Reading DATA csv ||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe is loaded.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(csv_directory)  \n",
    "\n",
    "# print(df,\"\\n\")              #displaying csv\n",
    "\n",
    "Ind, X_inlabel, Y_inlabel = [], [], []\n",
    "\n",
    "Ind = df[\"index\"]           #to store the image numbers excluding translational changes\n",
    "X_label = df[\"incl\"]        #to store the inclination angle\n",
    "Y_label = df[\"posang\"]      #to store the position angle\n",
    "\n",
    "print(\"The dataframe is loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||  Reading and loading DATA images ||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of images = 21000. ----> 100.00 %\n",
      "21000 image diretories are loaded.\n"
     ]
    }
   ],
   "source": [
    "## Reading the Image Dataset, from specified folders\n",
    "\n",
    "X = [] \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# a List to store oriented images\n",
    "Y = [] \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# a List to store face-on image\n",
    "\n",
    "k=0\n",
    "m=0\n",
    "\n",
    "print(f\"Total number of folders to be loaded is {no_o_folders}.\\n\")\n",
    "\n",
    "for i in range(1, no_o_folders+1):\n",
    "\n",
    "    directory = image_directory +\"/RT_A_\"+ str(i)+\"/*.png\"\n",
    "    data_set_indiv = glob.glob(directory)\n",
    "\n",
    "    loc = image_directory +\"/RT_A_\"+ str(i)\n",
    "    \n",
    "    for j in Ind: #Loading the oriented images in X and correspondsing face-on images in Y\n",
    "        X.append(loc+\"/image_\"+str(j)+\".png\")\n",
    "        Y.append(loc+\"/image_1.png\")\n",
    "        m = m+1\n",
    "    \n",
    "    k = k+1\n",
    "    display.clear_output(wait=True)\n",
    "    print(\"Total count of images = %d. ----> %2.2f %s\"%(m,(k/no_o_folders)*100,'%'))\n",
    "\n",
    "print(len(X), \"image diretories are loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display all the loaded images \n",
    "if (False):\n",
    "    fig, axes = plt.subplots(ncols=1, sharex=False,sharey=True, figsize=(15, 5))\n",
    "\n",
    "    k=0\n",
    "    for i in X:\n",
    "        try:\n",
    "            axes.set_title(\"Run:{}\".format(k))\n",
    "            plt.imshow(cv2.cvtColor(cv2.imread(i),cv2.COLOR_BGR2RGB))\n",
    "            k=k+1\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "            time.sleep(0.001)\n",
    "            if(k%2 == 0):\n",
    "                fig, axes = plt.subplots(ncols=1, sharex=False,sharey=True, figsize=(15, 5))\n",
    "        except KeyboardInterrupt:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1787"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deallocating the unreferenced objects and freeing up memory (OPTIONAL)\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 18900 \n",
      "Test: 2100 \n",
      "TrainLabel: 18900 \n",
      "TestLabel: 2100\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset\n",
    "trainX, testX, trainy, testy = train_test_split(X, Y, random_state=42, test_size=0.10, shuffle=True)\n",
    "\n",
    "# summarize the shape of the dataset\n",
    "print('Train:', len(trainX), '\\nTest:', len(testX), '\\nTrainLabel:', len(trainy), '\\nTestLabel:', len(testy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Training Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting a batch of random samples, returns images and target\n",
    "def generate_real_samples(dataset, n_samples, patch_shape):\n",
    "    # unpack dataset\n",
    "    trainA, trainB = dataset\n",
    "    # choose random instances\n",
    "    ix = randint(0, trainA.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X1, X2 = trainA[ix], trainB[ix//(len(Ind)-1)]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, patch_shape, patch_shape, 1))\n",
    "    return [X1, X2], y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a batch of images, returns images and targets\n",
    "def generate_fake_samples(g_model, samples, patch_shape):\n",
    "    # generate fake instance\n",
    "    X = g_model.predict(samples)\n",
    "    # create 'fake' class labels (0)\n",
    "    y = zeros((len(X), patch_shape, patch_shape, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading a batch of images\n",
    "def dataset_batch(lower, upper):\n",
    "    TX = []\n",
    "    TY = []\n",
    "    p = upper\n",
    "\n",
    "    for i in range(lower, upper):\n",
    "        if(i==len(trainX)):\n",
    "            p = i\n",
    "            break\n",
    "        image_dir = trainX[i]\n",
    "        emag = cv2.imread(image_dir, channel//3)\n",
    "        if(channel==1): emag = np.expand_dims(emag, axis=-1)\n",
    "        TX.append(emag[57:428, 107:478])\n",
    "        emag = cv2.flip(emag[57:428, 107:478],1)\n",
    "        if(channel==1): emag = np.expand_dims(emag, axis=-1)\n",
    "        TX.append(emag)\n",
    "        \n",
    "        trg_dir = trainy[i]\n",
    "        emag = cv2.imread(trg_dir, channel//3)\n",
    "        if(channel==1): emag = np.expand_dims(emag, axis=-1)\n",
    "        TY.append(emag[57:428, 107:478])\n",
    "        emag = cv2.flip(emag[57:428, 107:478],1)\n",
    "        if(channel==1): emag = np.expand_dims(emag, axis=-1)\n",
    "        TY.append(emag)\n",
    "        \n",
    "    print(\"Loading batch [%d --> %d] (%d)\" % (lower,p,p-lower))\n",
    "    \n",
    "    tx = tf.image.resize(np.asarray(TX), [pxl, pxl])\n",
    "    ty = tf.image.resize(np.asarray(TY), [pxl, pxl])\n",
    "\n",
    "    T_X = tx.numpy()\n",
    "    T_Y = ty.numpy()\n",
    "    \n",
    "    # scale from [0,255] to [-1,1]\n",
    "    T_X = (T_X - 127.5) / 127.5\n",
    "    T_Y = (T_Y - 127.5) / 127.5\n",
    "\n",
    "    return [T_X,T_Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating samples and saving plot and the model \n",
    "def summarize_performance(epch, g_model, dataset, direct, n_samples=3):\n",
    "    # select a sample of input images\n",
    "    [X_realA, X_realB] = dataset\n",
    "    \n",
    "    # generate a batch of fake samples\n",
    "    X_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
    "    \n",
    "    # scale all pixels from [-1,1] to [0,1]\n",
    "    X_fakeB = (X_fakeB + 1) / 2.0\n",
    "    \n",
    "    # plot real source images\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(3, n_samples, 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_realA[i]*255)\n",
    "    # plot generated target image\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(3, n_samples, 1 + n_samples + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_fakeB[i]*255)\n",
    "    # plot real target image\n",
    "    for i in range(n_samples):\n",
    "        plt.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(X_realB[i]*255)\n",
    "    \n",
    "    # save plot to file\n",
    "    formatted_date = (dt.now()).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    FD = formatted_date[2:10]+\"_\"+formatted_date[11:13]+\"_\"+formatted_date[14:16]\n",
    "        \n",
    "    filename1 = 'plot_%02d_%s.png' % ((epch), FD)\n",
    "    plt.savefig(f\"{direct}/{filename1}\")\n",
    "    plt.close()\n",
    "    # save the generator model\n",
    "    filename2 = 'modelWeight_%02d_%s.h5' % ((epch), FD)\n",
    "    g_model.save_weights(f\"{direct}/{filename2}\") \n",
    "    print('>Saved: %s and %s' % (filename1, filename2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pix2pix models\n",
    "def train(d_model, g_model, gan_model, dataset, n_epochs, n_batch):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    last_time = start_time\n",
    "    space = \" \"\n",
    "    lowest_rec = 100\n",
    "    highest_rec = 0\n",
    "    \n",
    "    # determine the output square shape of the discriminator\n",
    "    n_patch = d_model.output_shape[1]\n",
    "    \n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = ceil(len(trainX) / n_batch)\n",
    "    \n",
    "    # calculate the number of training iterations\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    \n",
    "    direct = \"\"      # to set up directory\n",
    "    boolean = True   # flag variable to make sure that the directory is set only once\n",
    "    \n",
    "    counting_batch_per_epoch = 1\n",
    "    dataset_index = 0\n",
    "    \n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        \n",
    "        #_____________________________________________________________________________Training Summary\n",
    "        print(f\"{space*65}______________________________________\\n\")\n",
    "        print(f\"{space*65}Epoch                    : {i//bat_per_epo +1} / {n_epochs}\")\n",
    "        print(f\"{space*65}Steps per epoch          : {counting_batch_per_epoch} / {bat_per_epo}\")\n",
    "        #_____________________________________________________________________________Training Summary\n",
    "        \n",
    "        #taking batch\n",
    "        dataset = dataset_batch(dataset_index, dataset_index+n_batch)\n",
    "        dataset_index = dataset_index+n_batch\n",
    "        \n",
    "        print(\"Total loaded images\", len(dataset[0]))\n",
    "        \n",
    "        \n",
    "        if(counting_batch_per_epoch==bat_per_epo):\n",
    "            dataset_index = 0\n",
    "            counting_batch_per_epoch = 0\n",
    "            \n",
    "        \n",
    "        # selecting a batch of real samples\n",
    "        [X_realA, X_realB], y_real = dataset, ones((len(dataset[0]), n_patch, n_patch, 1)) #generate_real_samples(dataset, n_patch)\n",
    "        # generate a batch of fake samples\n",
    "        X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
    "        \n",
    "\n",
    "        # updating discriminator for real samples\n",
    "        d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
    "        # update discriminator for generated samples\n",
    "        d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
    "        # update the generator\n",
    "        g_loss, _, _ = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
    "        \n",
    "        # summarizing performance\n",
    "        display.clear_output(wait=True) # ensures a single line print instead of multiple lines (OPTIONAL)\n",
    "        print(('>%d, d1[%.3f] d2[%.3f] g[%.3f] --->%2.2f %s ' % (i+1, d_loss1, d_loss2, g_loss, (i+1)/n_steps*100, '%')))\n",
    "        \n",
    "        \n",
    "        #_____________________________________________________________________________Training Summary\n",
    "        current_time = time.time()\n",
    "        current_time_elapsed = current_time - start_time\n",
    "        time_left = (current_time - last_time)*(n_steps-i-1)\n",
    "        print(f\"{space*65}Time Elaspsed{space*12}: {int(current_time_elapsed//3600)}h {int((current_time_elapsed%3600)//60)}m {int(current_time_elapsed%60)}s\\n{space*65}Approximate Time Left{space*4}: {int(time_left//3600)}h {int((time_left%3600)//60)}m {int(time_left%60)}s\")\n",
    "        \n",
    "        \n",
    "        if((current_time - last_time)>highest_rec): highest_rec = current_time - last_time\n",
    "        if((current_time - last_time)<lowest_rec): lowest_rec = current_time - last_time\n",
    "        \n",
    "        print(f\"{space*65}______________________________________\\n\")\n",
    "        print(f\"{space*65}\\033[1mTIME PER STEP\\033[0m\")\n",
    "        print(f\"%sLowest recorded          : %.2fs\"%(space*65,lowest_rec))\n",
    "        print(f\"%sCurrent time          ---> \\033[1m%.2fs\\033[0m\"%(space*65,current_time - last_time))\n",
    "        print(f\"%sHighest recorded         : %.2fs\"%(space*65,highest_rec))\n",
    "        \n",
    "        \n",
    "        last_time = current_time\n",
    "        #_____________________________________________________________________________Training Summary\n",
    "        \n",
    "        \n",
    "        # summarizing model performance\n",
    "        if (i+1) % (bat_per_epo * save_interval) == 0: # saving model and plot at every 5th epoch\n",
    "            print('Summarizing and Saving')\n",
    "            if(boolean):\n",
    "                print(\"Setting up directory...\")\n",
    "                formatted_date = (dt.now()).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                FD = \"ModelWeight_\"+formatted_date[2:10]+\"_\"+formatted_date[11:13]+\"_\"+formatted_date[14:16]\n",
    "                direct = \"/Users/Dyutiman/Documents/ML_Project/Pix2Pix/%s\"%(FD)\n",
    "                os.mkdir(direct) #creating directory\n",
    "                boolean = False\n",
    "            summarize_performance(i//bat_per_epo +1, g_model, dataset, direct)\n",
    "            gc.collect() #freeing up memory (OPTIONAL)\n",
    "            \n",
    "        counting_batch_per_epoch=counting_batch_per_epoch+1\n",
    "            \n",
    "            \n",
    "            \n",
    "#_____________ALERT_________________# auditory indication of the end of execution (OPTIONAL)\n",
    "    from IPython.display import Audio\n",
    "    sound_file = \"/Users/Dyutiman/Downloads/terminating alarm.mp3\"\n",
    "    Audio(sound_file, autoplay=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is (128, 128, 1)\n",
      "Working so far : d\n",
      "Working so far : g\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tensorflow2/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/tensorflow2/lib/python3.9/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# defining input shape based on the loaded dataset\n",
    "image_shape = (pxl,pxl,channel)\n",
    "print(\"Shape is\",image_shape)\n",
    "\n",
    "# define the models\n",
    "print(\"Working so far : d\")\n",
    "d_model = NN.define_discriminator(image_shape)\n",
    "\n",
    "print(\"Working so far : g\")\n",
    "g_model = NN.define_generator(image_shape)\n",
    "\n",
    "# define the composite model\n",
    "gan_model = NN.define_gan(g_model, d_model, image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Block preparation complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">27594, d1[0.051] d2[0.085] g[2.449] --->40.88 % \n",
      "                                                                 Time Elaspsed            : 17h 26m 18s\n",
      "                                                                 Approximate Time Left    : 25h 43m 44s\n",
      "                                                                 ______________________________________\n",
      "\n",
      "                                                                 \u001b[1mTIME PER STEP\u001b[0m\n",
      "                                                                 Lowest recorded          : 2.20s\n",
      "                                                                 Current time          ---> \u001b[1m2.32s\u001b[0m\n",
      "                                                                 Highest recorded         : 4.27s\n",
      "                                                                 ______________________________________\n",
      "\n",
      "                                                                 Epoch                    : 21 / 50\n",
      "                                                                 Steps per epoch          : 595 / 1350\n",
      "Loading batch [8316 --> 8330] (14)\n",
      "Total loaded images 28\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m [trainX, trainy]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# n_batch is the number images to be loaded everytime (the number is doubled as mirror-images are also generated)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgan_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m14\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#_____________ALERT_________________# auditory indication of the end of execution (OPTIONAL)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n",
      "Cell \u001b[0;32mIn [26], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(d_model, g_model, gan_model, dataset, n_epochs, n_batch)\u001b[0m\n\u001b[1;32m     55\u001b[0m d_loss2 \u001b[38;5;241m=\u001b[39m d_model\u001b[38;5;241m.\u001b[39mtrain_on_batch([X_realA, X_fakeB], y_fake)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# update the generator\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m g_loss, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mgan_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_realA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43my_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_realB\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# summarizing performance\u001b[39;00m\n\u001b[1;32m     60\u001b[0m display\u001b[38;5;241m.\u001b[39mclear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# ensures a single line print instead of multiple lines (OPTIONAL)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow2/lib/python3.9/site-packages/keras/engine/training.py:2383\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   2380\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m   2381\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m-> 2383\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict:\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logs\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow2/lib/python3.9/site-packages/keras/utils/tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 635\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/util/nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    914\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 917\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    918\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow2/lib/python3.9/site-packages/keras/utils/tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 628\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \n\u001b[1;32m   1136\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tensorflow2/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1122\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1124\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = [trainX, trainy]\n",
    "\n",
    "# n_batch is the number images to be loaded everytime (the number is doubled as mirror-images are also generated)\n",
    "\n",
    "train(d_model, g_model, gan_model, dataset, n_epochs=50, n_batch=14)\n",
    "\n",
    "\n",
    "#_____________ALERT_________________# auditory indication of the end of execution (OPTIONAL)\n",
    "from IPython.display import Audio\n",
    "sound_file = sound_file_directory\n",
    "Audio(sound_file, autoplay=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of code!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow) NEW",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
