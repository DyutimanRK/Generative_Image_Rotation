{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLvhBZS8hm2V"
   },
   "source": [
    "# Generative Network to rotate and obtain face-on PPD images\n",
    "## SOURCE CODE\n",
    "### Network Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    This script is dedicated to defining the architecture of a Conditional Generative Adversarial Network (cGAN), specifically implementing the Pix2Pix model. The network is designed to be trained on a dataset of Protoplanetary Disk images obtained from the FARGO3D simulations. The primary goal of the network is to generate face-on images of Protoplanetary Disks from images that are rotated in random orientations.\n",
    "\n",
    "    \n",
    "    Summary       : The purpose of this script is to establish the network architecture for leveraging the \n",
    "                    power of cGANs, particularly the Pix2Pix framework, to transform images of Protoplanetary \n",
    "                    Disks with random orientations into standardized face-on images. This architectural design\n",
    "                    is crucial for various astronomical studies and simulations, where consistent orientation \n",
    "                    of disk images is required for accurate analysis.\n",
    "\n",
    "\n",
    "    Code&Config   : The code is being done on Jupyter Notebook platform, and is being run on MacOS 13.1, \n",
    "                    Apple M1, 8gb configuration.  \n",
    "               \n",
    "         \n",
    "________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervisor   : Dr. Sayantan Auddy\n",
    "#### Written by    : Dyutiman Santra\n",
    "#### Created       : 24th June, 2024\n",
    "\n",
    "_______________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "||  Importing Modules  ||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fUE7t0Oyhngi"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers import BatchNormalization, Activation, Input, Dense, Conv2D, Conv2DTranspose, Flatten, Lambda, Reshape, Dropout, LeakyReLU, Embedding, Concatenate, Multiply, Add\n",
    "from keras.models import Model\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "from keras.initializers import RandomNormal\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#*****************\n",
    "\n",
    "from numpy import ones\n",
    "from numpy import zeros\n",
    "from numpy.random import randint\n",
    "\n",
    "#*****************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator, Encoder, Decoder, GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_discriminator(image_shape):\n",
    "    \n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02) \n",
    "    \n",
    "    # source image input\n",
    "    in_src_image = Input(shape=image_shape)  #Image we want to convert to another image\n",
    "    # target image input\n",
    "    in_target_image = Input(shape=image_shape)  #Image we want to generate after training. \n",
    "    \n",
    "    # concatenate images, channel-wise\n",
    "    merged = Concatenate()([in_src_image, in_target_image])\n",
    "    \n",
    "    # C64: 4x4 kernel Stride 2x2\n",
    "    d = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C128: 4x4 kernel Stride 2x2\n",
    "    d = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C256: 4x4 kernel Stride 2x2\n",
    "    d = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C512: 4x4 kernel Stride 2x2 \n",
    "    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # C512 2nd: 4x4 kernel Stride 2x2 \n",
    "    d = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    # second last output layer : 4x4 kernel but Stride 1x1\n",
    "    d = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    d = BatchNormalization()(d)\n",
    "    d = LeakyReLU(alpha=0.2)(d)\n",
    "    \n",
    "    # patch output\n",
    "    d = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
    "    patch_out = Activation('sigmoid')(d)\n",
    "    # define model\n",
    "    model = Model([in_src_image, in_target_image], patch_out)\n",
    "    \n",
    "    # compile model\n",
    "    #The model is trained with a batch size of one image and Adam opt. with a small learning rate and 0.5 beta. \n",
    "    #The loss for the discriminator is weighted by 50% for each model update.\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
    "    return model\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Defining the generator - a U-net\n",
    "# defining an encoder block to be used in generator\n",
    "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
    "    #print(layer_in.shape)\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # add downsampling layer\n",
    "    g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    # conditionally add batch normalization\n",
    "    if batchnorm:\n",
    "        g = BatchNormalization()(g, training=True)\n",
    "    # leaky relu activation\n",
    "    g = LeakyReLU(alpha=0.2)(g)\n",
    "    return g\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# define a decoder block to be used in generator\n",
    "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # add upsampling layer\n",
    "    g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
    "    # add batch normalization\n",
    "    g = BatchNormalization()(g, training=True)\n",
    "    # conditionally add dropout\n",
    "    if dropout:\n",
    "        g = Dropout(0.5)(g, training=True)\n",
    "    # merge with skip connection\n",
    "    g = Concatenate()([g, skip_in])\n",
    "    # relu activation\n",
    "    g = Activation('relu')(g)\n",
    "    return g\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model, image_shape):\n",
    "    # make weights in the discriminator not trainable\n",
    "    for layer in d_model.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False      \n",
    "\n",
    "    # define the source image\n",
    "    in_src = Input(shape=image_shape)\n",
    "    # suppy the image as input to the generator \n",
    "    gen_out = g_model(in_src)\n",
    "    # supply the input image and generated image as inputs to the discriminator\n",
    "    dis_out = d_model([in_src, gen_out])\n",
    "    # src image as input, generated image and disc. output as outputs\n",
    "    model = Model(in_src, [dis_out, gen_out])\n",
    "    # compile model\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "    #Total loss is the weighted sum of adversarial loss (BCE) and L1 loss (MAE)\n",
    "    model.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define encoder block with attention\n",
    "def define_encoder_block_with_attention(layer_in, n_filters, batchnorm=True):\n",
    "    # Encoder block as before\n",
    "    g = define_encoder_block(layer_in, n_filters, batchnorm=batchnorm)\n",
    "    # Downsample input tensor to match the spatial dimensions of attention gate output\n",
    "    skip_in = Conv2D(n_filters, (1, 1), strides=(2, 2), padding='same')(layer_in)\n",
    "    # Apply attention mechanism\n",
    "    attention = attention_gate(skip_in, g, n_filters // 2)\n",
    "    # Concatenate attention and encoder block output\n",
    "    g = Concatenate()([g, attention])\n",
    "    return g\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Define decoder block with attention\n",
    "def decoder_block_with_attention(layer_in, skip_in, n_filters, dropout=True):\n",
    "    # Decoder block as before\n",
    "    g = decoder_block(layer_in, skip_in, n_filters, dropout=dropout)\n",
    "    # Apply attention mechanism\n",
    "    attention = attention_gate(g, skip_in, n_filters // 2)\n",
    "    # Concatenate attention and decoder block output\n",
    "    g = Concatenate()([g, attention])\n",
    "    return g\n",
    "\n",
    "########################################################################################################################\n",
    "\n",
    "# Attention gate\n",
    "def attention_gate(x, g, inter_channels):\n",
    "    # Inter-channel attention\n",
    "    theta_x = Conv2D(inter_channels, kernel_size=1, strides=1, padding='same')(x)\n",
    "    phi_g = Conv2D(inter_channels, kernel_size=1, strides=1, padding='same')(g)\n",
    "    f = Activation('relu')(Add()([theta_x, phi_g]))\n",
    "    psi_f = Conv2D(1, kernel_size=1, strides=1, padding='same')(f)\n",
    "    rate = Activation('sigmoid')(psi_f)\n",
    "\n",
    "    # Apply attention gate\n",
    "    attention = Multiply()([x, rate])\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the standalone generator model - U-net\n",
    "def define_generator(image_shape):      \n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image input\n",
    "    in_image = Input(shape=image_shape)\n",
    "\n",
    "    e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
    "    e2 = define_encoder_block(e1, 128)\n",
    "    e3 = define_encoder_block(e2, 256)\n",
    "    e4 = define_encoder_block(e3, 512)\n",
    "    e5 = define_encoder_block(e4, 512)\n",
    "#         e6 = define_encoder_block(e5, 512)\n",
    "#         e7 = define_encoder_block(e6, 512) #this should be included if images are 256x256 (e6, e7, b,d1, d2 should be accordingly placed in places)\n",
    "#         e8 = define_encoder_block(e7, 512) #this should be included if images are 512x512\n",
    "    # bottleneck, no batch norm and relu\n",
    "    b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e5) #this would be (e7) in case of 256x256\n",
    "    b = Activation('relu')(b)\n",
    "\n",
    "#         d0 = decoder_block(b, e8, 512) #this should be included if images are 512x512\n",
    "#         d1 = decoder_block(b, e7, 512) #this should be included if images are 256x256\n",
    "#         d2 = decoder_block(d1, e6, 512) #the b will change to d1 if resolution is increased\n",
    "    d3 = decoder_block(b, e5, 512)\n",
    "    d4 = decoder_block(d3, e4, 512, dropout=False)\n",
    "    d5 = decoder_block(d4, e3, 256, dropout=False)\n",
    "    d6 = decoder_block(d5, e2, 128, dropout=False)\n",
    "    d7 = decoder_block(d6, e1, 64, dropout=False)\n",
    "\n",
    "    # output\n",
    "    g = Conv2DTranspose(image_shape[2], (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7) #Modified \n",
    "    out_image = Activation('tanh')(g)  #Generates images in the range -1 to 1. So we change inputs also to -1 to 1\n",
    "\n",
    "    # define model\n",
    "    model = Model(in_image, out_image)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the standalone generator model - U-net with attention\n",
    "def define_generator_with_attention(image_shape):\n",
    "\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # image input\n",
    "    in_image = Input(shape=image_shape)\n",
    "\n",
    "    e1 = define_encoder_block_with_attention(in_image, 64, batchnorm=False)\n",
    "    e2 = define_encoder_block_with_attention(e1, 128)\n",
    "    e3 = define_encoder_block_with_attention(e2, 256)\n",
    "    e4 = define_encoder_block_with_attention(e3, 512)\n",
    "    e5 = define_encoder_block_with_attention(e4, 512)\n",
    "#         e6 = define_encoder_block_with_attention(e5, 512)\n",
    "#         e7 = define_encoder_block_with_attention(e6, 512)\n",
    "    b = Conv2D(512, (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(e5)\n",
    "    b = Activation('relu')(b)\n",
    "#         d1 = decoder_block_with_attention(b, e7, 512)\n",
    "#         d2 = decoder_block_with_attention(d1, e6, 512)\n",
    "    d3 = decoder_block_with_attention(b, e5, 512)\n",
    "    d4 = decoder_block_with_attention(d3, e4, 512, dropout=False)\n",
    "    d5 = decoder_block_with_attention(d4, e3, 256, dropout=False)\n",
    "    d6 = decoder_block_with_attention(d5, e2, 128, dropout=False)\n",
    "    d7 = decoder_block_with_attention(d6, e1, 64, dropout=False)\n",
    "\n",
    "    g = Conv2DTranspose(image_shape[2], (4, 4), strides=(2, 2), padding='same', kernel_initializer=init)(d7)\n",
    "    out_image = Activation('tanh')(g)\n",
    "\n",
    "    model = Model(in_image, out_image)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Setup complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow) NEW",
   "language": "python",
   "name": "tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
